{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_TEXT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gazabAnurag/TF_BETARUNS/blob/master/LSTM_TEXT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "4nBDoqWl8Epk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "c2c9654c-9081-4ae4-9ec9-0afce7e5be4c"
      },
      "cell_type": "code",
      "source": [
        "!pip install unicode"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unicode\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/f8/7e3975ec60aeeb06dda1b633b9c4063eaececbf5c393ecda97104ee9fc9a/unicode-2.6-py2.py3-none-any.whl\n",
            "Installing collected packages: unicode\n",
            "Successfully installed unicode-2.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7uRdrQP48IaQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import unidecode\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cPE4nUah8qsI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f8767b8c-7d13-4496-d44d-62bd5f6f53e1"
      },
      "cell_type": "code",
      "source": [
        "name,path='shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
        "file = tf.keras.utils.get_file(name,path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VoLhSrmz9J_l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "85751e1c-fb5b-444c-b880-3e53616b8954"
      },
      "cell_type": "code",
      "source": [
        "text = unidecode.unidecode(open(file).read())\n",
        "len(text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "iM4jvoLF9dAV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "textsorted = sorted(set(text))\n",
        "idx2chr = {k:v for (k,v) in enumerate(textsorted)}\n",
        "chr2idx = {v:k for (k,v) in enumerate(textsorted)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r4H8H10j9pw_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_length = 150\n",
        "vocab_size = len(textsorted)\n",
        "embedding_dim = 256\n",
        "gru_units=1024\n",
        "BATCH_SIZE=64\n",
        "BUFFER_SIZE= 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IvS7cHEY-S8x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#create batches for input and output tensor\n",
        "input_text = []\n",
        "output_text = []\n",
        "for i in range(0,len(text)-max_length,max_length):\n",
        "  temp_in= text[i:i+max_length]\n",
        "  temp_out= text[i+1:i+1+max_length]\n",
        "  input_text.append([chr2idx[i] for i in temp_in])\n",
        "  output_text.append([chr2idx[i] for i in temp_out])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tJnkO5KtBUGo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8832ccf-d2d8-4c98-beb2-9bcb4c970866"
      },
      "cell_type": "code",
      "source": [
        "len(np.array(output_text).shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "D6_si0r6BxL7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = tf.data.Dataset.from_tensor_slices((input_text,output_text)).shuffle(BUFFER_SIZE)\n",
        "data = data.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LDboOk3lDeQT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b3e4d5d7-69ee-41ec-f4ca-f8dc957ecc7c"
      },
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((64, 150), (64, 150)), types: (tf.int32, tf.int32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E9FG5Z6CEZEl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model(tf.keras.Model):\n",
        "  def __init__(self,vocab_size,embedding_dim,units,batch_size):\n",
        "    super(Model,self).__init__()\n",
        "    self.units=units\n",
        "    self.batch_sz=batch_size\n",
        "    self.embedding =tf.keras.layers.Embedding(vocab_size,embedding_dim)\n",
        "    #check if the GPU is availaible using tf.test.is_gpu_available()\n",
        "    if tf.test.is_gpu_available():\n",
        "      self.gru = tf.keras.layers.CuDNNGRU(self.units, \n",
        "                                          return_sequences=True, \n",
        "                                          return_state=True, \n",
        "                                          recurrent_initializer='glorot_uniform')\n",
        "    else:\n",
        "      self.gru = tf.keras.layers.GRU(self.units, \n",
        "                                     return_sequences=True, \n",
        "                                     return_state=True, \n",
        "                                     recurrent_activation='sigmoid', \n",
        "                                     recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "  def call(self,x,hidden):\n",
        "    x = self.embedding(x)\n",
        "    output,state = self.gru(x,initial_state = hidden)\n",
        "    output = tf.reshape(output,(-1,output.shape[2]))\n",
        "    x= self.fc(output)\n",
        "    return x,state\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BkpCspKlNTNF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Model(vocab_size,embedding_dim,gru_units,BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sCka-xttOEhS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.AdamOptimizer()\n",
        "def loss_function(real,pred):\n",
        "  return tf.losses.sparse_softmax_cross_entropy(labels=real,logits=pred)\n",
        "\n",
        "checkpoint_dir = './train_checkpoint'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir,\"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,model=model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tXXXi64dQKog",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1745
        },
        "outputId": "908e7944-7d63-46bd-873e-eb01c3239075"
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    # initializing the hidden state at the start of every epoch\n",
        "    hidden = model.reset_states()\n",
        "    \n",
        "    for (batch, (inp, target)) in enumerate(data):\n",
        "          with tf.GradientTape() as tape:\n",
        "              # feeding the hidden state back into the model\n",
        "              # This is the interesting step\n",
        "              predictions, hidden = model(inp, hidden)\n",
        "              \n",
        "              # reshaping the target because that's how the \n",
        "              # loss function expects it\n",
        "              target = tf.reshape(target, (-1,))\n",
        "              loss = loss_function(target, predictions)\n",
        "              \n",
        "          grads = tape.gradient(loss, model.variables)\n",
        "          optimizer.apply_gradients(zip(grads, model.variables))\n",
        "\n",
        "          if batch % 100 == 0:\n",
        "              print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1,\n",
        "                                                            batch,\n",
        "                                                            loss))\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.1750\n",
            "Epoch 1 Batch 100 Loss 2.3121\n",
            "Epoch 1 Loss 2.2693\n",
            "Time taken for 1 epoch 25.119115591049194 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.2700\n",
            "Epoch 2 Batch 100 Loss 2.0042\n",
            "Epoch 2 Loss 1.9599\n",
            "Time taken for 1 epoch 22.764246940612793 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.9052\n",
            "Epoch 3 Batch 100 Loss 1.7613\n",
            "Epoch 3 Loss 1.7419\n",
            "Time taken for 1 epoch 22.606393814086914 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.7334\n",
            "Epoch 4 Batch 100 Loss 1.6387\n",
            "Epoch 4 Loss 1.6016\n",
            "Time taken for 1 epoch 23.487173557281494 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.5910\n",
            "Epoch 5 Batch 100 Loss 1.5231\n",
            "Epoch 5 Loss 1.5267\n",
            "Time taken for 1 epoch 22.91302800178528 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.4725\n",
            "Epoch 6 Batch 100 Loss 1.4577\n",
            "Epoch 6 Loss 1.4419\n",
            "Time taken for 1 epoch 22.95051860809326 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.3990\n",
            "Epoch 7 Batch 100 Loss 1.4161\n",
            "Epoch 7 Loss 1.3781\n",
            "Time taken for 1 epoch 22.668579578399658 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.3425\n",
            "Epoch 8 Batch 100 Loss 1.3671\n",
            "Epoch 8 Loss 1.3624\n",
            "Time taken for 1 epoch 22.68913722038269 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.3399\n",
            "Epoch 9 Batch 100 Loss 1.3254\n",
            "Epoch 9 Loss 1.3155\n",
            "Time taken for 1 epoch 22.722858905792236 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.2842\n",
            "Epoch 10 Batch 100 Loss 1.2971\n",
            "Epoch 10 Loss 1.2592\n",
            "Time taken for 1 epoch 22.99060583114624 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.2372\n",
            "Epoch 11 Batch 100 Loss 1.2378\n",
            "Epoch 11 Loss 1.2594\n",
            "Time taken for 1 epoch 22.55241346359253 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.1758\n",
            "Epoch 12 Batch 100 Loss 1.2534\n",
            "Epoch 12 Loss 1.2575\n",
            "Time taken for 1 epoch 22.54176425933838 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.2063\n",
            "Epoch 13 Batch 100 Loss 1.2348\n",
            "Epoch 13 Loss 1.2175\n",
            "Time taken for 1 epoch 22.418589115142822 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.1489\n",
            "Epoch 14 Batch 100 Loss 1.1712\n",
            "Epoch 14 Loss 1.1913\n",
            "Time taken for 1 epoch 22.43424963951111 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.0886\n",
            "Epoch 15 Batch 100 Loss 1.1446\n",
            "Epoch 15 Loss 1.1799\n",
            "Time taken for 1 epoch 22.606029510498047 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.0789\n",
            "Epoch 16 Batch 100 Loss 1.1428\n",
            "Epoch 16 Loss 1.1698\n",
            "Time taken for 1 epoch 23.530524492263794 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.0583\n",
            "Epoch 17 Batch 100 Loss 1.1006\n",
            "Epoch 17 Loss 1.1075\n",
            "Time taken for 1 epoch 22.71646761894226 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.0070\n",
            "Epoch 18 Batch 100 Loss 1.0449\n",
            "Epoch 18 Loss 1.0677\n",
            "Time taken for 1 epoch 22.575028896331787 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.9745\n",
            "Epoch 19 Batch 100 Loss 1.0422\n",
            "Epoch 19 Loss 1.0559\n",
            "Time taken for 1 epoch 22.620192050933838 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.9407\n",
            "Epoch 20 Batch 100 Loss 1.0064\n",
            "Epoch 20 Loss 1.0110\n",
            "Time taken for 1 epoch 22.518871545791626 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N59XFyEXQpa5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "78575e36-b8c2-4f89-ef16-9e90d49668d3"
      },
      "cell_type": "code",
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7fe2c43868d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "u6UM6u5nW239",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "outputId": "b9194c8d-f156-42b9-bf34-13913e1d0c3f"
      },
      "cell_type": "code",
      "source": [
        "# Evaluation step(generating text using the model learned)\n",
        "\n",
        "# number of characters to generate\n",
        "num_generate = 1000\n",
        "\n",
        "# You can change the start string to experiment\n",
        "start_string = 'Q'\n",
        "# converting our start string to numbers(vectorizing!) \n",
        "input_eval = [chr2idx[s] for s in start_string]\n",
        "input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "# empty string to store our results\n",
        "text_generated = ''\n",
        "\n",
        "# hidden state shape == (batch_size, number of rnn units); here batch size == 1\n",
        "hidden = [tf.zeros((1, gru_units))]\n",
        "for i in range(num_generate):\n",
        "    predictions, hidden = model(input_eval, hidden)\n",
        "\n",
        "    # using argmax to predict the word returned by the model\n",
        "    predicted_id = tf.argmax(predictions[-1]).numpy()\n",
        "    \n",
        "    # We pass the predicted word as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    \n",
        "    text_generated += idx2chr[predicted_id]\n",
        "\n",
        "print (start_string + text_generated)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "QUEY:\n",
            "I have done this world to hear thee hence,\n",
            "And there the court of the morning of a fool.\n",
            "\n",
            "GLOUCESTER:\n",
            "Why, then he shall not stay to thee and the princess of the courtier.\n",
            "\n",
            "Shepherd:\n",
            "What, will you go with me?\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I know you well, so hath the fair days of the world,\n",
            "The manner of your son, the thing I have the more\n",
            "Than when the sense of honour and the court\n",
            "And so he came from him as he hath done thee how to say\n",
            "When he shall be the fair daughter of a king,\n",
            "And then he says he is coming.\n",
            "\n",
            "BRUTUS:\n",
            "We have a man to do the gods;\n",
            "And then have ta'en the streets as free from hence,\n",
            "And there the court of the morning of a fool.\n",
            "\n",
            "GLOUCESTER:\n",
            "Why, then he shall not stay to thee and the princess of the courtier.\n",
            "\n",
            "Shepherd:\n",
            "What, will you go with me?\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I know you well, so hath the fair days of the world,\n",
            "The manner of your son, the thing I have the more\n",
            "Than when the sense of honour and the court\n",
            "And so he came from him as he hath done thee how to say\n",
            "When he sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "43n7QEa3XIR1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}